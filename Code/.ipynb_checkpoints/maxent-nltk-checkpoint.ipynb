{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROCESSED_FILE = '../train-processed.csv'\n",
    "TEST_PROCESSED_FILE = '../test-processed.csv'\n",
    "USE_BIGRAMS = False\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file_name, isTrain=True):\n",
    "    data = []\n",
    "    with open(train_csv_file, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if isTrain:\n",
    "                tag = line.split(',')[1]\n",
    "                bag_of_words = line.split(',')[2].split()\n",
    "                if USE_BIGRAMS:\n",
    "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[2].split()))\n",
    "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
    "            else :\n",
    "                tag = '5'\n",
    "                bag_of_words = line.split(',')[1].split()\n",
    "                if USE_BIGRAMS:\n",
    "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[1].split()))\n",
    "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
    "            data.append((bag_of_words, tag))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(tweets, validation_split=0.1):\n",
    "    index = int((1 - validation_split) * len(tweets))\n",
    "    random.shuffle(tweets)\n",
    "    return tweets[:index], tweets[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict(words_list):\n",
    "    return dict([(word, True) for word in words_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.479\n",
      "         Final          -0.51010        0.901\n",
      "   1.000 2==True and label is '1'\n",
      "   1.000 twittersmalfunctioningagain==True and label is '0'\n",
      "   1.000 appleupdates==True and label is '0'\n",
      "   0.500 chao==True and label is '1'\n",
      "   0.500 cu==True and label is '0'\n",
      "\n",
      "Validation set accuracy:0.7841\n",
      "\n",
      "Predicting for test data\n",
      "\n",
      "Saved to maxent.csv\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "np.random.seed(1337)\n",
    "train_csv_file = TRAIN_PROCESSED_FILE\n",
    "test_csv_file = TEST_PROCESSED_FILE\n",
    "train_data = get_data_from_file(train_csv_file, isTrain=True)\n",
    "train_set, validation_set = split_data(train_data)\n",
    "#print(train_set[1])\n",
    "training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in train_set]\n",
    "#print(training_set_formatted[1])\n",
    "validation_set_formatted = [(list_to_dict(element[0]), element[1]) for element in validation_set]\n",
    "numIterations = 1\n",
    "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[1]\n",
    "#print(algorithm)\n",
    "classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)\n",
    "classifier.show_most_informative_features(5)\n",
    "count = int(0)\n",
    "for review in validation_set_formatted:\n",
    "    label = review[1]\n",
    "    text = review[0]\n",
    "    determined_label = classifier.classify(text)\n",
    "    #print(determined_label, label)\n",
    "    if determined_label!=label:\n",
    "        count+=int(1)\n",
    "accuracy = (len(validation_set)-count)/len(validation_set)\n",
    "print()\n",
    "print('Validation set accuracy:%.4f'% (accuracy))\n",
    "f = open('maxEnt_classifier.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()\n",
    "print('\\nPredicting for test data')\n",
    "test_data = get_data_from_file(test_csv_file, isTrain=False)\n",
    "test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in test_data]\n",
    "tweet_id = int(0)\n",
    "results = []\n",
    "for review in test_set_formatted:\n",
    "    text = review[0]\n",
    "    label = classifier.classify(text)\n",
    "    results.append((str(tweet_id), label))\n",
    "    tweet_id += int(1)\n",
    "#save_results_to_csv(results, 'maxent.csv')\n",
    "print('\\nSaved to maxent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
